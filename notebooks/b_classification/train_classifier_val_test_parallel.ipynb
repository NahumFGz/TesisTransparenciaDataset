{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "Memoria total: 15.99169921875 GB\n",
      "Memoria usada: 0.0 GB\n",
      "Memoria reservada: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(\"Memoria total:\", torch.cuda.get_device_properties(0).total_memory / 1024**3, \"GB\")\n",
    "    print(\"Memoria usada:\", torch.cuda.memory_allocated() / 1024**3, \"GB\")\n",
    "    print(\"Memoria reservada:\", torch.cuda.memory_reserved() / 1024**3, \"GB\")\n",
    "else:\n",
    "    print(\"CUDA no disponible.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_size(model_name: str):\n",
    "    \"\"\"\n",
    "    Retorna el tama√±o de entrada esperado por el modelo.\n",
    "    \"\"\"\n",
    "    name = model_name.lower()\n",
    "    if name in [\n",
    "        'vgg16', 'resnet50', 'densenet121',\n",
    "        'mobilenet_v2', 'googlenet', 'efficientnet_b0'\n",
    "    ]:\n",
    "        return (224, 224)\n",
    "    elif name == 'inception_v3':\n",
    "        return (299, 299)\n",
    "    else:\n",
    "        return (224, 224)  # valor por defecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name: str, num_classes: int):\n",
    "    \"\"\"\n",
    "    Carga y retorna un modelo preentrenado con la √∫ltima capa ajustada a 'num_classes'.\n",
    "    \"\"\"\n",
    "    name = model_name.lower()\n",
    "\n",
    "    # ResNet50\n",
    "    if name == 'resnet50':\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    # VGG16\n",
    "    elif name == 'vgg16':\n",
    "        model = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "        model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "\n",
    "    # DenseNet121\n",
    "    elif name == 'densenet121':\n",
    "        model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "\n",
    "    # MobileNet v2\n",
    "    elif name == 'mobilenet_v2':\n",
    "        model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "    # EfficientNet b0\n",
    "    elif name == 'efficientnet_b0':\n",
    "        model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "    # Inception v3\n",
    "    elif name == 'inception_v3':\n",
    "        model = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT, aux_logits=False)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    # GoogLeNet\n",
    "    elif name == 'googlenet':\n",
    "        model = models.googlenet(weights=models.GoogLeNet_Weights.DEFAULT, aux_logits=False)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Modelo '{model_name}' no est√° implementado.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(\n",
    "    data_dir: str, \n",
    "    model_name: str, \n",
    "    batch_size: int, \n",
    "    train_fraction: float = 0.8, \n",
    "    val_fraction: float = 0.1, \n",
    "    test_fraction: float = 0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Crea DataLoaders para entrenamiento, validaci√≥n y test a partir de las im√°genes en 'data_dir'.\n",
    "    \n",
    "    Por defecto:\n",
    "      - train_fraction = 0.8 (80%)\n",
    "      - val_fraction   = 0.1 (10%)\n",
    "      - test_fraction  = 0.1 (10%)\n",
    "\n",
    "    Se asume que train_fraction + val_fraction + test_fraction = 1.0\n",
    "    \"\"\"\n",
    "    # Verificamos que la suma sea 1.0 (o muy cercana, por ejemplo 0.9999...1.0001)\n",
    "    total = train_fraction + val_fraction + test_fraction\n",
    "    assert abs(total - 1.0) < 1e-8, \"La suma de train, val y test fractions debe ser 1.0.\"\n",
    "\n",
    "    input_size = get_input_size(model_name)\n",
    "\n",
    "    # Transformaciones de entrada\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Dataset completo\n",
    "    dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "    num_samples = len(dataset)\n",
    "    num_classes = len(dataset.classes)\n",
    "\n",
    "    # Calcular tama√±os\n",
    "    train_size = int(num_samples * train_fraction)\n",
    "    val_size = int(num_samples * val_fraction)\n",
    "    test_size = num_samples - train_size - val_size  # lo que quede\n",
    "\n",
    "    # Split: train, val, test\n",
    "    train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_data,   batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_data,  batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Eval√∫a el modelo en un dataloader (por ejemplo, test_loader)\n",
    "    y retorna la p√©rdida y accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    total_samples = len(dataloader.dataset)\n",
    "    final_loss = running_loss / total_samples\n",
    "    final_acc = running_corrects.double() / total_samples\n",
    "\n",
    "    return final_loss, final_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model_name: str,\n",
    "    data_dir: str,\n",
    "    batch_size: int = 32,\n",
    "    num_epochs: int = 30,\n",
    "    lr: float = 1e-4,\n",
    "    train_fraction: float = 0.8,\n",
    "    val_fraction: float = 0.1,\n",
    "    test_fraction: float = 0.1,\n",
    "    device: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Funci√≥n principal para entrenar un modelo con triple partici√≥n (train, val, test):\n",
    "      - Carga los DataLoaders\n",
    "      - Construye el modelo\n",
    "      - Realiza entrenamiento (train) y validaci√≥n (val)\n",
    "      - Finalmente eval√∫a en test\n",
    "      - Retorna el modelo entrenado y un historial de m√©tricas para graficar\n",
    "    \"\"\"\n",
    "    # Seleccionar dispositivo\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "    # 1) Obtener DataLoaders y n√∫mero de clases\n",
    "    train_loader, val_loader, test_loader, num_classes = get_dataloaders(\n",
    "        data_dir=data_dir,\n",
    "        model_name=model_name,\n",
    "        batch_size=batch_size,\n",
    "        train_fraction=train_fraction,\n",
    "        val_fraction=val_fraction,\n",
    "        test_fraction=test_fraction\n",
    "    )\n",
    "\n",
    "    # 2) Construir modelo\n",
    "    model = get_model(model_name, num_classes).to(device)\n",
    "\n",
    "    # 3) Definir criterio de p√©rdida y optimizador\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Para llevar registro del historial\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nüì¶ Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()\n",
    "                dataloader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            loop = tqdm(dataloader, desc=f\"{phase.capitalize()} Phase\")\n",
    "            for inputs, labels in loop:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Habilitar/Deshabilitar gradientes seg√∫n la fase\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Acumular m√©tricas\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "            if phase == \"train\":\n",
    "                train_loss_history.append(epoch_loss)\n",
    "                train_acc_history.append(epoch_acc.item())\n",
    "            else:\n",
    "                val_loss_history.append(epoch_loss)\n",
    "                val_acc_history.append(epoch_acc.item())\n",
    "\n",
    "            print(f\"{phase} Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "            # Guardar el mejor modelo en funci√≥n de la val_acc\n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(f\"\\n‚úÖ Mejor accuracy en validaci√≥n: {best_acc:.4f}\")\n",
    "\n",
    "    # Cargar mejores pesos\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # 4) Guardar modelo entrenado\n",
    "    save_name = f\"{model_name}_best_parallel.pth\"\n",
    "    torch.save(model.state_dict(), save_name)\n",
    "    print(f\"Modelo guardado como: {save_name}\")\n",
    "\n",
    "    # 5) Evaluar en el conjunto de test\n",
    "    test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "    print(f\"\\n=== Evaluaci√≥n en Test ===\\nTest Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    # Retornamos el modelo y la historia de entrenamiento\n",
    "    return model, (train_loss_history, train_acc_history, val_loss_history, val_acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(train_loss, train_acc, val_loss, val_acc):\n",
    "    \"\"\"\n",
    "    Crea dos gr√°ficas:\n",
    "    1. P√©rdida (loss) vs. √©poca para train y val\n",
    "    2. Exactitud (accuracy) vs. √©poca para train y val\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    # Gr√°fico de p√©rdida\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_loss, label='Train Loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Gr√°fico de exactitud\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_acc, label='Train Accuracy')\n",
    "    plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy over epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "\n",
      "üì¶ Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Phase:  23%|‚ñà‚ñà‚ñé       | 81/349 [00:22<01:14,  3.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m TEST_FRACTION \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Entrenar modelo\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m trained_model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAIN_FRACTION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVAL_FRACTION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEST_FRACTION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Desempaquetar historia\u001b[39;00m\n\u001b[1;32m     27\u001b[0m train_loss_history, train_acc_history, val_loss_history, val_acc_history \u001b[38;5;241m=\u001b[39m history\n",
      "Cell \u001b[0;32mIn[7], line 81\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model_name, data_dir, batch_size, num_epochs, lr, train_fraction, val_fraction, test_fraction, device)\u001b[0m\n\u001b[1;32m     78\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Acumular m√©tricas\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     82\u001b[0m     running_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     84\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Par√°metros de ejemplo\n",
    "MODEL_NAME = \"mobilenet_v2\"\n",
    "DATA_DIR = \"/home/nahumfg/Projects/GithubProjects/TesisTransparenciaDataset/data/a_org\"\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "LR = 2e-4\n",
    "\n",
    "# Queremos 80% train, 10% val, 10% test\n",
    "TRAIN_FRACTION = 0.8\n",
    "VAL_FRACTION = 0.1\n",
    "TEST_FRACTION = 0.1\n",
    "\n",
    "# Entrenar modelo\n",
    "trained_model, history = train_model(\n",
    "    model_name=MODEL_NAME,\n",
    "    data_dir=DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    lr=LR,\n",
    "    train_fraction=TRAIN_FRACTION,\n",
    "    val_fraction=VAL_FRACTION,\n",
    "    test_fraction=TEST_FRACTION,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# Desempaquetar historia\n",
    "train_loss_history, train_acc_history, val_loss_history, val_acc_history = history\n",
    "\n",
    "# Graficar\n",
    "plot_training_curves(\n",
    "    train_loss=train_loss_history,\n",
    "    train_acc=train_acc_history,\n",
    "    val_loss=val_loss_history,\n",
    "    val_acc=val_acc_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tesisdataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
